import datetime
from langchain_core.messages import SystemMessage, HumanMessage
from utils.rate_limiter import rate_limited_llm_call
from core.vectorstore.retriever import retrieve_documents, get_context_from_documents
from typing import Dict, Optional
from langchain_chroma import Chroma

def create_evaluation_agent(llm, vectorstores: Dict[str, Chroma]):
    """
    Crea un agente especializado en evaluaciones educativas que:
    1. Prioriza los criterios especificados por el usuario
    2. Usa configuraci√≥n por defecto cuando no se especifican criterios
    3. Se adapta al nivel y asignatura
    """
    system_prompt = """Eres un experto en evaluaci√≥n educativa chilena. Tu tarea es crear evaluaciones que:

    1. PRIORICEN LOS CRITERIOS DEL USUARIO:
       - N√∫mero de preguntas especificado
       - Tipos de preguntas solicitados
       - Temas o contenidos espec√≠ficos
       - Formato de evaluaci√≥n requerido
       - Mes del a√±o escolar (para ajustar dificultad seg√∫n progresi√≥n)

    2. CONFIGURACI√ìN POR DEFECTO (si no se especifica):
       - 8 preguntas de selecci√≥n m√∫ltiple
       - 2 preguntas de desarrollo
       - Distribuci√≥n de dificultad:
         * 40% nivel b√°sico
         * 40% nivel intermedio
         * 20% nivel avanzado

    3. ESTRUCTURA DE LA EVALUACI√ìN:
       SELECCI√ìN M√öLTIPLE:
       - Enunciado claro y preciso
       - 4 alternativas por pregunta
       - Solo una respuesta correcta
       - Distractores plausibles
       
       DESARROLLO:
       - Instrucciones detalladas
       - Espacio adecuado para respuesta
       - R√∫brica de evaluaci√≥n
       - Puntajes asignados

    4. ELEMENTOS ADICIONALES:
       - Encabezado completo
       - Instrucciones generales
       - Tiempo sugerido
       - Puntaje total y de aprobaci√≥n
       - Tabla de especificaciones

    5. CONSIDERACIONES:
       - ADAPTA LA DIFICULTAD SEG√öN EL NIVEL EDUCATIVO (1¬∞ b√°sico = 6 a√±os hasta 4¬∞ medio = 18 a√±os)
       - PROGRESI√ìN MENSUAL EJEMPLO: el contenido de abril debe ser m√°s avanzado que marzo y el de mayo m√°s avanzado que abril.
       - Alinear con objetivos de aprendizaje seg√∫n mes del a√±o escolar
       - Adaptar lenguaje al nivel
       - Incluir contextos significativos
       - Evaluar diferentes habilidades
       - Permitir demostrar comprensi√≥n

    IMPORTANTE: 
    - PRIORIZAR SIEMPRE los criterios espec√≠ficos del usuario
    - Usar configuraci√≥n por defecto solo cuando no hay especificaciones
    - Incluir retroalimentaci√≥n para cada pregunta
    - Proporcionar r√∫bricas detalladas
    - Si la evaluaci√≥n es para meses espec√≠ficos, RESPETAR LA PROGRESI√ìN DEL APRENDIZAJE
    """

    def evaluation_agent_executor(query: str,
                                asignatura: Optional[str] = None,
                                nivel: Optional[str] = None,
                                mes: Optional[str] = None):
        """
        Ejecuta el agente de evaluaci√≥n.
        
        Args:
            query: Consulta del usuario
            asignatura: Asignatura objetivo
            nivel: Nivel educativo
            mes: Mes del a√±o escolar
        """
        # Verificar que tengamos acceso a los vectorstores necesarios
        required_categories = ["bases curriculares", "orientaciones", "propuesta", "actividades sugeridas"]
        missing_categories = [cat for cat in required_categories if cat not in vectorstores]
        if missing_categories:
            return (f"‚ö†Ô∏è No se encontraron algunas categor√≠as necesarias: {', '.join(missing_categories)}. "
                   "Por favor, verifica la disponibilidad de los documentos.", False, None)

        # Verificar informaci√≥n faltante
        faltante = []
        if not asignatura:
            # Intentar extraer asignatura de la consulta
            extract_prompt = [
                SystemMessage(
                    content="Extrae la asignatura mencionada en esta solicitud. Si no hay ninguna, responde 'No especificada'."),
                HumanMessage(content=query)
            ]
            asignatura_result = rate_limited_llm_call(
                llm.invoke, extract_prompt)
            if "No especificada" in asignatura_result.content:
                faltante.append("asignatura")
            else:
                asignatura = asignatura_result.content.strip()

        if not nivel:
            # Intentar extraer nivel de la consulta
            extract_prompt = [
                SystemMessage(
                    content="Extrae el nivel educativo mencionado en esta solicitud (ej: 5¬∞ b√°sico, 2¬∞ medio). Si no hay ninguno, responde 'No especificado'."),
                HumanMessage(content=query)
            ]
            nivel_result = rate_limited_llm_call(llm.invoke, extract_prompt)
            if "No especificado" in nivel_result.content:
                faltante.append("nivel")
            else:
                nivel = nivel_result.content.strip()
        
        # Si no se ha especificado un mes, usamos el mes actual
        if not mes:
            meses = ["enero", "febrero", "marzo", "abril", "mayo", "junio", 
                    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"]
            mes_actual = datetime.datetime.now().month
            mes = meses[mes_actual - 1]
            print(f"\nüìÖ Usando mes actual para evaluaci√≥n: {mes}")

        # Si falta informaci√≥n, solicitarla
        if faltante:
            response = "Para crear una evaluaci√≥n educativa completa, necesito la siguiente informaci√≥n:\n\n"
            if "asignatura" in faltante:
                response += "- ¬øPara qu√© asignatura necesitas la evaluaci√≥n? (Lenguaje, Matem√°ticas, etc.)\n"
            if "nivel" in faltante:
                response += "- ¬øPara qu√© nivel educativo? (Ej: 2¬∞ b√°sico, 8¬∞ b√°sico, 3¬∞ medio, etc.)\n"
            return response, True, {"asignatura": asignatura, "nivel": nivel, "mes": mes}

        try:
            # Construir query enriquecida
            enhanced_query = (
                f"evaluaci√≥n {asignatura} {nivel} {mes} "
                f"objetivos aprendizaje indicadores logro contenidos preguntas instrumento evaluaci√≥n"
            )

            # Definir prioridad de b√∫squeda por categor√≠as
            priority_categories = ["bases curriculares", "orientaciones", "propuesta", "actividades sugeridas"]
            
            print(f"\nüîç Buscando informaci√≥n relevante en: {', '.join(priority_categories)}")
            
            # Recuperar documentos relevantes
            retrieved_docs = retrieve_documents(
                vectorstores=vectorstores,
                query=enhanced_query,
                categories=priority_categories,
                k=7
            )

            if not retrieved_docs:
                return ("No se encontr√≥ informaci√≥n suficiente en los documentos curriculares. "
                       "Por favor, verifica los criterios de b√∫squeda.", False, None)

            # Extraer contexto y fuentes
            context, sources = get_context_from_documents(retrieved_docs)
            source_text = ", ".join(sources) if sources else "documentos curriculares disponibles"

            print(f"üìö Fuentes consultadas: {source_text}")

            # Generar la evaluaci√≥n
            evaluation_prompt = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"""
                SOLICITUD: {query}
                ASIGNATURA: {asignatura}
                NIVEL: {nivel}
                MES: {mes}

                CONTEXTO CURRICULAR:
                {context}

                FUENTES CONSULTADAS:
                {source_text}

                Por favor, genera una evaluaci√≥n que:
                1. Se adapte al nivel y asignatura espec√≠ficos
                2. Use la configuraci√≥n por defecto para aspectos no especificados
                3. Se alinee con el curr√≠culum nacional
                4. Incluya instrucciones claras y r√∫bricas de evaluaci√≥n
                5. Sea apropiada para el mes de {mes} en el calendario escolar
                """)
            ]

            response = rate_limited_llm_call(llm.invoke, evaluation_prompt)
            return response.content, False, {"asignatura": asignatura, "nivel": nivel, "mes": mes}

        except Exception as e:
            print(f"‚ùå Error en evaluation_agent: {e}")
            return ("Ocurri√≥ un error al generar la evaluaci√≥n. Por favor, intenta nuevamente.",
                   False, None)

    return evaluation_agent_executor